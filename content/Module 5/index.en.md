---
title : "Module 4 : Extend the LLM inference beyond AWS through Remote Inference"
weight : 500
---

This lab will focus on Integrating Inference APIs which are Open AI API compatible. We will go through the changes required in EKS deployment and changes in OPEA pipeline. This lab will use the pre-installed inference APIs in [Denvr Dataworks](https://www.denvrdata.com/intel), which were enabled using [Intel Gaudi2 AI Accelerator](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html) instances. This lab provides an option for users to move the APIs to managed services and showcases that OPEA inference services (such as vLLM and TGI) can be hosted in multiple cloud providers and build a seamless pipeline.

**Learning Objectives**

* Understand the OPEA changes for integrating managed Inference services.
* Explore Intel Gaudi AI Accelerators
* OPEA powered Inference services in Denvr Dataworks cloud
* Evaluate the response quality for different models for quality and performance.

